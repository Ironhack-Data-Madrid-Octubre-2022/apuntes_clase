{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.5 - PySpark\n",
    "\n",
    "$$$$\n",
    "\n",
    "![pyspark](images/pyspark.jpg)\n",
    "\n",
    "$$$$\n",
    "\n",
    "Apache Spark es un framework de computación en clúster open-source. Fue desarrollada originariamente en la Universidad de California, en el AMPLab de Berkeley. El código base del proyecto Spark fue donado más tarde a la Apache Software Foundation que se encarga de su mantenimiento desde entonces. Spark proporciona una interfaz para la programación de clusters completos con Paralelismo de Datos implícito y tolerancia a fallos.\n",
    "\n",
    "Apache Spark se puede considerar un sistema de computación en clúster de propósito general y orientado a la velocidad. Proporciona APIs en Java, Scala, Python y R. También proporciona un motor optimizado que soporta la ejecución de grafos en general. También soporta un conjunto extenso y rico de herramientas de alto nivel entre las que se incluyen Spark SQL (para el procesamiento de datos estructurados basada en SQL), MLlib para implementar machine learning, GraphX para el procesamiento de grafos y Spark Streaming."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install findspark\n",
    "%pip install pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter('ignore')\n",
    "\n",
    "import findspark\n",
    "findspark.init() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark=SparkSession.builder.appName('Nombre').getOrCreate()  # inicia la sesion de spark\n",
    "\n",
    "path='../data/student-por.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=spark.read.csv(path, header=True, inferSchema=True, sep=';')\n",
    "\n",
    "data.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(data.show(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_cols=['school', 'sex', 'age', 'Mjob', 'Fjob', 'reason', 'guardian']\n",
    "\n",
    "data=data.select([c for c in data.columns if c not in drop_cols])\n",
    "\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_numeric_columns=[item[0] for item in data.dtypes if item[1].startswith('string')]\n",
    "\n",
    "non_numeric_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "struct_data=data.select('*')\n",
    "\n",
    "struct_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StringIndexer\n",
    "from pyspark.sql.types import IntegerType\n",
    "\n",
    "\n",
    "indexers=StringIndexer(inputCols=non_numeric_columns, \n",
    "                       outputCols=[c+'_' for c in non_numeric_columns],\n",
    "                       stringOrderType='alphabetAsc')\n",
    "\n",
    "struct_data=indexers.fit(struct_data).transform(struct_data)\n",
    "\n",
    "struct_data=struct_data.select([c for c in struct_data.columns if c not in non_numeric_columns])\n",
    "\n",
    "for c in struct_data.columns:\n",
    "    struct_data=struct_data.withColumn(c, struct_data[c].cast(IntegerType()))\n",
    "\n",
    "    \n",
    "struct_data.toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejemplo: aproximando $\\pi$\n",
    "\n",
    "Utilizaremos el Método de MonteCarlo para aproximar el número $\\pi$. El método Monte Carlo es un método en el que por medio de la estadística y la probabilidad podemos determinar valores o soluciones de ecuaciones que calculados con exactitud son muy complejas, pero que mediante este método resulta sencillo calcular una aproximación al resultado que buscamos.\n",
    "\n",
    "$$$$\n",
    "\n",
    "![pi](images/pi.png)\n",
    "\n",
    "$$$$\n",
    "\n",
    "Lo primero construir el entorno de trabajo. Este sería:\n",
    "\n",
    "+ Construiremos un cuadrado de lado 1.\n",
    "+ Construimos un círculo inscrito en el cuadrado, que tiene de centro, el centro del cuadrado y de radio 1. Su área será $\\pi$.\n",
    "+ Generaremos puntos al azar dentro del cuadrado. Para entenderlo mejor es como lanzar dardos sobre una diana con los ojos vendados, de tal forma que siempre acertamos dentro de los límites de ese cuadrado. \n",
    "\n",
    "Aplicamos ahora el Método MonteCarlo:\n",
    "+ Contaremos el total de puntos generados.\n",
    "+ Contaremos el total de puntos que cayeron dentro del círculo.\n",
    "+ Realizaremos el siguiente razonamiento:\n",
    "\n",
    "$$A0 =  Área_{cuadrado} = N_{puntos} $$\n",
    "$$$$\n",
    "$$A1 = Área_{círculo} = \\pi · r^{2}$$\n",
    "\n",
    "Ahora:\n",
    "\n",
    "$$\\frac{\\pi · r^{2}}{N_{puntos}} = \\frac{Área_{círculo}}{Área_{cuadrado}}$$\n",
    "\n",
    "Resumiremos en un cuadrante, y los que nos queda es que:\n",
    "\n",
    "$$\\pi=4·Área_{cuadrante}$$\n",
    "\n",
    "El valor de $\\pi$ es 4 veces la probabilidad de que el punto caiga en la zona roja."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from pyspark import SparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# puntos aleatorios dentor del círculo\n",
    "\n",
    "def dentro(punto):\n",
    "    x, y = np.random.random(), np.random.random()\n",
    "    return x*x + y*y < 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimar_pi(n_total):\n",
    "    print('Proceso normal...')\n",
    "\n",
    "    puntos=list(filter(dentro, list(range(n_total)))) \n",
    "    \n",
    "    cuenta=len(puntos)\n",
    "  \n",
    "    return 4. * cuenta/n_total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "display(estimar_pi(5000))\n",
    "display(estimar_pi(50000))\n",
    "display(estimar_pi(5000000))\n",
    "        \n",
    "display('Valor real pi: ' ,np.pi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**con spark**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sesion=SparkContext.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimar_pi_paralelo(n_total):\n",
    "    print('Proceso con Spark..')\n",
    "\n",
    "    cuenta=sesion.parallelize(range(0, n_total)).filter(dentro).count()\n",
    "\n",
    "    return 4. * cuenta/n_total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "display(estimar_pi_paralelo(5000))\n",
    "display(estimar_pi_paralelo(50000))\n",
    "display(estimar_pi_paralelo(5000000))\n",
    "        \n",
    "display('Valor real pi: ' ,np.pi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "prueba",
   "language": "python",
   "name": "prueba"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
